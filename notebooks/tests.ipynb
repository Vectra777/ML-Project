{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il6QeumjpDtc",
        "outputId": "1bee54bc-be85-48fd-b1bd-89ba7c62419b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.utils import img_to_array, to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Tuple, List\n",
        "\n",
        "# ====================================================================\n",
        "#                      CONFIGURATIONS ET CONSTANTES\n",
        "# ====================================================================\n",
        "EPOCHS             = 25\n",
        "INIT_LR            = 1e-3\n",
        "BS                 = 16\n",
        "WIDTH              = 256\n",
        "HEIGHT             = 256\n",
        "DEFAULT_IMAGE_SIZE = tuple((WIDTH, HEIGHT))\n",
        "DEPTH              = 3\n",
        "DATASET_REF        = \"emmarex/plantdisease\"\n",
        "WORKING_DIR_NAME   = \"plantdisease_working\" # R√©pertoire modifiable pour la copie\n",
        "ALLOWED_EXTS       = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\"}\n",
        "MAX_PER_CLASS      = 100000  # Limite le nombre d'images par classe\n",
        "\n",
        "# ====================================================================\n",
        "#                           FONCTIONS UTILITAIRES\n",
        "# ====================================================================\n",
        "\n",
        "\n",
        "def _find_class_dirs(root: Path) -> List[Path]:\n",
        "    \"\"\"Trouve tous les dossiers qui contiennent des images et les traite comme des classes.\"\"\"\n",
        "    class_dirs = []\n",
        "    for p in root.iterdir():\n",
        "        if p.is_dir() and any(f.is_file() and f.suffix.lower() in ALLOWED_EXTS for f in p.iterdir()):\n",
        "            class_dirs.append(p)\n",
        "    return sorted(class_dirs, key=lambda x: x.name)\n",
        "\n",
        "def consolidate_dataset(read_only_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Copie le dataset vers un r√©pertoire modifiable et supprime les chemins dupliqu√©s.\n",
        "    Ceci est essentiel pour r√©soudre l'erreur \"Read-only file system\".\n",
        "    \"\"\"\n",
        "    print(f\"\\n## üîÑ 2. Copie vers R√©pertoire Modifiable et Nettoyage\")\n",
        "\n",
        "    # D√©finition et cr√©ation du chemin de travail (modifiable)\n",
        "    working_base_dir = os.path.join(os.getcwd(), WORKING_DIR_NAME)\n",
        "    if os.path.exists(working_base_dir):\n",
        "        shutil.rmtree(working_base_dir)\n",
        "\n",
        "    # Copie l'int√©gralit√© du dataset\n",
        "    print(f\"   -> Copie de {read_only_path} vers {working_base_dir}...\")\n",
        "    shutil.copytree(read_only_path, working_base_dir)\n",
        "    base_dir = working_base_dir\n",
        "\n",
        "    # Nettoyage des doublons de chemin (ex: plantdisease_working/plantvillage/PlantVillage)\n",
        "    target_root_name = \"PlantVillage\"\n",
        "    target_root = os.path.join(base_dir, target_root_name)\n",
        "    duplicated_root = os.path.join(base_dir, \"plantvillage\", target_root_name)\n",
        "\n",
        "    if os.path.exists(duplicated_root):\n",
        "        print(f\"üí° Dossiers dupliqu√©s trouv√©s. Consolidation en cours...\")\n",
        "        try:\n",
        "            for class_name in os.listdir(duplicated_root):\n",
        "                source_folder = os.path.join(duplicated_root, class_name)\n",
        "                target_folder = os.path.join(target_root, class_name)\n",
        "\n",
        "                if os.path.isdir(source_folder):\n",
        "                    # Assurer que le dossier cible existe\n",
        "                    if not os.path.exists(target_folder):\n",
        "                        os.makedirs(target_folder)\n",
        "\n",
        "                    # D√©placer les fichiers (shutil.move est maintenant possible)\n",
        "                    for item_name in os.listdir(source_folder):\n",
        "                        source_item = os.path.join(source_folder, item_name)\n",
        "                        target_item = os.path.join(target_folder, item_name)\n",
        "\n",
        "                        if os.path.isfile(source_item) and item_name.endswith(tuple(ALLOWED_EXTS)):\n",
        "                             shutil.move(source_item, target_item)\n",
        "\n",
        "            # Supprimer la structure de dossiers vide et dupliqu√©e\n",
        "            shutil.rmtree(os.path.join(base_dir, \"plantvillage\"))\n",
        "            print(\"‚úÖ Consolidation termin√©e. \")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors du d√©placement des fichiers : {e}\")\n",
        "\n",
        "    return os.path.join(base_dir, 'PlantVillage') # Retourne le chemin du r√©pertoire des classes\n",
        "\n",
        "\n",
        "def plot_class_distribution_global(class_counts: dict):\n",
        "    \"\"\"Affiche la distribution des images par classe avant le split.\"\"\"\n",
        "    if not class_counts: return\n",
        "    labels = list(class_counts.keys())\n",
        "    counts = list(class_counts.values())\n",
        "\n",
        "    plt.style.use('ggplot')\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    ax = sns.barplot(x=labels, y=counts, palette=\"viridis\")\n",
        "    ax.set_title(f\"Distribution des images par classe (Limit√©e √† {MAX_PER_CLASS})\", fontsize=16)\n",
        "    ax.set_xlabel(\"Classe\", fontsize=12)\n",
        "    ax.set_ylabel(\"Nombre d'images\", fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    for index, value in enumerate(counts):\n",
        "        plt.text(index, value + (max(counts)*0.01), str(value), ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_class_distribution(labels: np.ndarray, name: str = \"Dataset\") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Affiche la distribution des classes en pourcentages.\"\"\"\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    print(f\"\\nClass distribution in '{name}':\")\n",
        "    for cls, count in zip(unique, counts):\n",
        "        print(f\"  {cls} : {count} ({count / len(labels) * 100:.2f}%)\")\n",
        "    return unique, counts\n",
        "\n",
        "# ====================================================================\n",
        "#                           LOGIQUE PRINCIPALE\n",
        "# ====================================================================\n",
        "\n",
        "# --- √âTAPE 1: T√©l√©chargement et Nettoyage du Dataset ---\n",
        "\n",
        "print(f\"## ‚¨áÔ∏è 1. T√©l√©chargement du Dataset ({DATASET_REF})\")\n",
        "try:\n",
        "    read_only_path = kagglehub.dataset_download(DATASET_REF)\n",
        "    print(f\"‚úÖ Dataset trouv√© (Lecture Seule) √† : {read_only_path}\")\n",
        "\n",
        "    # Nettoyage et copie vers le r√©pertoire modifiable\n",
        "    # DIRECTORY_ROOT sera le chemin du dossier PlantVillage dans le r√©pertoire de travail\n",
        "    DIRECTORY_ROOT = consolidate_dataset(read_only_path)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur critique : {e}\")\n",
        "    DIRECTORY_ROOT = None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wmM46MvMrPSC",
        "outputId": "ae285849-d088-4bde-863e-f7987081b8e7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from typing import Tuple, List\n",
        "import pickle\n",
        "\n",
        "# Imports Keras/TensorFlow corrig√©s (n√©cessaires pour to_categorical)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import Sequence # N√©cessaire pour la classe DataGenerator (m√™me si non utilis√©e dans ce bloc)\n",
        "\n",
        "# Imports Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Imports Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "#                      CONFIGURATIONS ET CONSTANTES\n",
        "# ====================================================================\n",
        "BS                 = 16\n",
        "WIDTH              = 256\n",
        "HEIGHT             = 256\n",
        "DEFAULT_IMAGE_SIZE = (WIDTH, HEIGHT)\n",
        "\n",
        "# **CHEMIN D'ACC√àS CL√â : V√âRIFIEZ ET CORRIGEZ CE CHEMIN**\n",
        "DIRECTORY_ROOT     = '/content/plantdisease_working/PlantVillage'\n",
        "\n",
        "ALLOWED_EXTS       = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
        "MAX_PER_CLASS      = 100000\n",
        "SAVE_DIR           = '/content/saved_artefacts'\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "#                           FONCTIONS UTILITAIRES\n",
        "# ====================================================================\n",
        "\n",
        "def _find_class_dirs(root: Path) -> List[Path]:\n",
        "    \"\"\"Trouve tous les dossiers qui contiennent des images et les traite comme des classes.\"\"\"\n",
        "    class_dirs = []\n",
        "    for p in root.iterdir():\n",
        "        if p.is_dir() and any(f.is_file() and f.suffix.lower() in ALLOWED_EXTS for f in p.iterdir()):\n",
        "            class_dirs.append(p)\n",
        "    return sorted(class_dirs, key=lambda x: x.name)\n",
        "\n",
        "def plot_class_distribution_global(class_counts: dict):\n",
        "    if not class_counts: return\n",
        "    labels = list(class_counts.keys())\n",
        "    counts = list(class_counts.values())\n",
        "    plt.style.use('ggplot')\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    ax = sns.barplot(x=labels, y=counts, palette=\"viridis\")\n",
        "    ax.set_title(f\"Distribution des images par classe (Limit√©e √† {MAX_PER_CLASS})\", fontsize=16)\n",
        "    ax.set_xlabel(\"Classe\", fontsize=12)\n",
        "    ax.set_ylabel(\"Nombre d'images\", fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    for index, value in enumerate(counts):\n",
        "        plt.text(index, value + (max(counts)*0.01), str(value), ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_class_distribution(labels: np.ndarray, name: str = \"Dataset\") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    print(f\"\\nClass distribution in '{name}':\")\n",
        "    for cls, count in zip(unique, counts):\n",
        "        print(f\"  {cls} : {count} ({count / len(labels) * 100:.2f}%)\")\n",
        "    return unique, counts\n",
        "\n",
        "def plot_class_distribution_split(global_labels: List[str], train_labels: List[str], test_labels: List[str]):\n",
        "    unique_classes = sorted(list(set(global_labels)))\n",
        "    def get_counts(labels):\n",
        "        counts = {cls: 0 for cls in unique_classes}\n",
        "        for label in labels:\n",
        "            if label in counts: counts[label] += 1\n",
        "        return np.array(list(counts.values()))\n",
        "\n",
        "    global_counts = get_counts(global_labels)\n",
        "    train_counts = get_counts(train_labels)\n",
        "    test_counts = get_counts(test_labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "    global_props = global_counts / global_counts.sum()\n",
        "    train_props = train_counts / train_counts.sum()\n",
        "    test_props = test_counts / test_counts.sum()\n",
        "    x = np.arange(len(unique_classes))\n",
        "    width = 0.25\n",
        "\n",
        "    ax.bar(x - width, global_props, width, label='Global (proportion)', alpha=0.8, color='grey')\n",
        "    ax.bar(x, train_props, width, label='Train (proportion)', alpha=0.8, color='green')\n",
        "    ax.bar(x + width, test_props, width, label='Test (proportion)', alpha=0.8, color='red')\n",
        "\n",
        "    ax.set_xlabel(\"Classes\")\n",
        "    ax.set_ylabel(\"Proportion\")\n",
        "    ax.set_title(\"Proportions des classes (Global vs Train vs Test) \")\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(unique_classes, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ====================================================================\n",
        "#                           LOGIQUE PRINCIPALE\n",
        "# ====================================================================\n",
        "\n",
        "# --- √âTAPE 1: Collecte des Chemins ---\n",
        "all_paths, all_labels = [], []\n",
        "per_class_counts = {}\n",
        "\n",
        "print(f\"## 1. üìÇ Collecte des Chemins de Fichiers (Faible RAM)\")\n",
        "\n",
        "if not os.path.exists(DIRECTORY_ROOT):\n",
        "    print(f\"‚ùå ERREUR: Le chemin sp√©cifi√© ({DIRECTORY_ROOT}) n'existe pas. Arr√™t.\")\n",
        "    exit()\n",
        "else:\n",
        "    root = Path(DIRECTORY_ROOT)\n",
        "    class_dirs = _find_class_dirs(root)\n",
        "\n",
        "    if not class_dirs:\n",
        "        print(f\"‚ùå ERREUR: Aucun dossier de classe trouv√© dans {root}. Arr√™t.\")\n",
        "        exit()\n",
        "\n",
        "    for cdir in class_dirs:\n",
        "        class_name = cdir.name\n",
        "        file_paths = []\n",
        "        for ext in ALLOWED_EXTS:\n",
        "            file_paths.extend(cdir.glob(f\"*{ext}\"))\n",
        "            file_paths.extend(cdir.glob(f\"*{ext.upper()}\"))\n",
        "\n",
        "        files_to_process = sorted(list(set(file_paths)))[:MAX_PER_CLASS]\n",
        "        per_class_counts[class_name] = len(files_to_process)\n",
        "\n",
        "        for fp in files_to_process:\n",
        "            all_paths.append(str(fp))\n",
        "            all_labels.append(class_name)\n",
        "\n",
        "    print(f\"[INFO] Total chemins collect√©s : {len(all_paths)}\")\n",
        "\n",
        "    # Cr√©er le DataFrame (cl√© pour la stratification)\n",
        "    df = pd.DataFrame({'path': all_paths, 'label': all_labels})\n",
        "\n",
        "    # Visualisation de la distribution globale\n",
        "    print(\"\\n## üìà 2. Visualisation de la Distribution Globale\")\n",
        "    plot_class_distribution_global(per_class_counts)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --- √âTAPE 2: Division Stratifi√©e des CHEMINS et Encodage des Labels ---\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n## 3. ‚úÇÔ∏è Division Stratifi√©e des Chemins et Encodage des Labels\")\n",
        "\n",
        "# 1. Pr√©paration de l'encodeur de labels\n",
        "label_binarizer = LabelBinarizer()\n",
        "labels_encoded_all = label_binarizer.fit_transform(df['label'])\n",
        "N_CLASSES = len(label_binarizer.classes_)\n",
        "CLASS_NAMES = label_binarizer.classes_\n",
        "\n",
        "# 2. Division Stratifi√©e des lignes du DataFrame\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=df['label'] # Stratifie sur les labels string\n",
        ")\n",
        "\n",
        "# 3. Encoder les labels pour les g√©n√©rateurs ou le chargement futur\n",
        "train_labels_encoded = label_binarizer.transform(train_df['label'])\n",
        "test_labels_encoded = label_binarizer.transform(test_df['label'])\n",
        "\n",
        "# 4. Conversion en One-Hot Encoding si multiclasses (√† stocker si n√©cessaire pour un futur chargement)\n",
        "if N_CLASSES > 2:\n",
        "    train_labels_encoded = to_categorical(train_labels_encoded)\n",
        "    test_labels_encoded = to_categorical(test_labels_encoded)\n",
        "\n",
        "\n",
        "print(f\"[INFO] Train samples : {len(train_df)}\")\n",
        "print(f\"[INFO] Test samples  : {len(test_df)}\")\n",
        "print(f\"[INFO] Nombre de classes : {N_CLASSES}\")\n",
        "\n",
        "# V√©rification de la stratification et trac√©\n",
        "print_class_distribution(np.array(all_labels), name=\"Total Dataset\")\n",
        "plot_class_distribution_split(all_labels, train_df['label'].tolist(), test_df['label'].tolist())\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# --- √âTAPE 3: Sauvegarde des Artefacts (DataFrames et LabelBinarizer) ---\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(f\"\\n## 4. üíæ Sauvegarde des Artefacts de Donn√©es\")\n",
        "\n",
        "# 1. Sauvegarde des DataFrames (chemins et labels non encod√©s)\n",
        "# Cela permet de recr√©er les g√©n√©rateurs/jeux de donn√©es exactement plus tard.\n",
        "train_df.to_csv(os.path.join(SAVE_DIR, 'train_df.csv'), index=False)\n",
        "test_df.to_csv(os.path.join(SAVE_DIR, 'test_df.csv'), index=False)\n",
        "print(\"‚úÖ DataFrames (chemins/labels) sauvegard√©s.\")\n",
        "\n",
        "# 2. Sauvegarde du LabelBinarizer\n",
        "BINARIZER_PATH = os.path.join(SAVE_DIR, 'label_binarizer.pkl')\n",
        "try:\n",
        "    with open(BINARIZER_PATH, 'wb') as f:\n",
        "        pickle.dump(label_binarizer, f)\n",
        "    print(f\"‚úÖ LabelBinarizer sauvegard√© avec succ√®s √† : {BINARIZER_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors de la sauvegarde du LabelBinarizer : {e}\")\n",
        "\n",
        "# 3. Sauvegarde des Noms des Classes\n",
        "CLASSES_PATH = os.path.join(SAVE_DIR, 'class_names.pkl')\n",
        "try:\n",
        "    with open(CLASSES_PATH, 'wb') as f:\n",
        "        pickle.dump(CLASS_NAMES, f)\n",
        "    print(f\"‚úÖ Noms des classes sauvegard√©s avec succ√®s √† : {CLASSES_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur lors de la sauvegarde des noms des classes : {e}\")\n",
        "\n",
        "print(\"\\n[FIN] Pipeline de pr√©paration des donn√©es termin√© et artefacts sauvegard√©s.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3cGUQNi1_4n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "#                      CONSTANTES ET R√âPLICATION\n",
        "# ====================================================================\n",
        "BS                 = 64\n",
        "WIDTH              = 256\n",
        "HEIGHT             = 256\n",
        "DEFAULT_IMAGE_SIZE = (WIDTH, HEIGHT)\n",
        "INIT_LR            = 1e-3\n",
        "EPOCHS             = 25\n",
        "SAVE_DIR           = '/content/saved_artefacts'\n",
        "\n",
        "# ====================================================================\n",
        "#           CLASSE G√âN√âRATEUR (Gestion de la RAM)\n",
        "# ====================================================================\n",
        "class DataGenerator(Sequence):\n",
        "    # ... (Votre classe DataGenerator reste inchang√©e) ...\n",
        "    \"\"\"G√©n√©rateur de donn√©es Keras pour charger les images par lots.\"\"\"\n",
        "\n",
        "    def __init__(self, df, labels_encoded, batch_size=BS, dim=DEFAULT_IMAGE_SIZE, n_channels=3,\n",
        "                 n_classes=None, shuffle=True):\n",
        "        self.df = df\n",
        "        self.labels_encoded = np.array(labels_encoded)\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = dim\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Nombre de lots par √©poque'\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'G√©n√®re un lot de donn√©es'\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        list_paths_temp = self.df['path'].iloc[indices].tolist()\n",
        "        labels_temp = self.labels_encoded[indices]\n",
        "        X, y = self.__data_generation(list_paths_temp, labels_temp)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'M√©lange les indices apr√®s chaque √©poque'\n",
        "        self.indices = np.arange(len(self.df))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, list_paths_temp, labels_temp):\n",
        "        'Charge et pr√©traite les images d un lot'\n",
        "\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float32)\n",
        "        y = labels_temp\n",
        "\n",
        "        for i, path in enumerate(list_paths_temp):\n",
        "            image = cv2.imread(path)\n",
        "            if image is not None:\n",
        "                image = cv2.resize(image, self.dim)\n",
        "                X[i,] = image.astype('float32') / 255.0\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "#                      CHARGEMENT DES ARTEFACTS\n",
        "# ====================================================================\n",
        "\n",
        "print(\"## 1. üîÑ Chargement des Artefacts de Donn√©es\")\n",
        "# ... (Chargement des DataFrames et du Binarizer - inchang√©)\n",
        "try:\n",
        "    loaded_train_df = pd.read_csv(os.path.join(SAVE_DIR, 'train_df.csv'))\n",
        "    loaded_test_df = pd.read_csv(os.path.join(SAVE_DIR, 'test_df.csv'))\n",
        "    print(\"‚úÖ DataFrames Train et Test charg√©s avec succ√®s.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå ERREUR: Fichiers CSV non trouv√©s dans {SAVE_DIR}. Veuillez ex√©cuter le script de pr√©paration avant.\")\n",
        "    exit()\n",
        "\n",
        "BINARIZER_PATH = os.path.join(SAVE_DIR, 'label_binarizer.pkl')\n",
        "try:\n",
        "    with open(BINARIZER_PATH, 'rb') as f:\n",
        "        loaded_binarizer = pickle.load(f)\n",
        "    N_CLASSES = len(loaded_binarizer.classes_)\n",
        "    print(f\"‚úÖ LabelBinarizer charg√©. {N_CLASSES} classes d√©tect√©es.\")\n",
        "except:\n",
        "    print(f\"‚ùå ERREUR: Le fichier label_binarizer.pkl n'a pas √©t√© trouv√©.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "#                      PR√âPARATION POUR L'ENTRA√éNEMENT\n",
        "# ====================================================================\n",
        "\n",
        "print(\"\\n## 2. Pr√©paration et Nettoyage des Labels\")\n",
        "\n",
        "# 1. Encoder les labels en utilisant le binariseur charg√©\n",
        "train_labels_encoded = loaded_binarizer.transform(loaded_train_df['label'])\n",
        "test_labels_encoded = loaded_binarizer.transform(loaded_test_df['label'])\n",
        "\n",
        "# --- CORRECTION CLEF ICI ---\n",
        "# Le LabelBinarizer produit des donn√©es de forme (N, 1) pour N_CLASSES=2, ou (N, N_CLASSES) pour N_CLASSES>2\n",
        "# Si les labels ont √©t√© mal sauvegard√©s, ils peuvent avoir des dimensions superflues.\n",
        "# On utilise .squeeze() pour enlever toute dimension de taille 1 (comme le 1 dans (N, 1))\n",
        "\n",
        "# Nettoyage des labels (enl√®ve les dimensions 1 superflues)\n",
        "train_labels_encoded = np.squeeze(train_labels_encoded)\n",
        "test_labels_encoded = np.squeeze(test_labels_encoded)\n",
        "\n",
        "print(f\"[DEBUG] Forme de 'train_labels_encoded' apr√®s binarisation/nettoyage : {train_labels_encoded.shape}\")\n",
        "\n",
        "# 2. Appliquer le One-Hot Encoding pour garantir la forme [N, N_CLASSES]\n",
        "# C'est essentiel pour 'categorical_crossentropy' et l'architecture CNN standard.\n",
        "if N_CLASSES >= 2:\n",
        "    # to_categorical prend l'array 1D d'indices ou l'array 2D (N, 1) et le convertit en [N, N_CLASSES]\n",
        "    # Nous devons reconstruire les labels √† partir des indices pour to_categorical\n",
        "\n",
        "    # Si les labels sont d√©j√† [N, N_CLASSES], ne rien faire.\n",
        "    # Si les labels sont [N,], on utilise to_categorical.\n",
        "    if train_labels_encoded.ndim == 1:\n",
        "        # Re-transformer les labels string en indices entiers (0, 1, 2, ...)\n",
        "        label_map = {name: i for i, name in enumerate(loaded_binarizer.classes_)}\n",
        "        train_indices = loaded_train_df['label'].map(label_map).values\n",
        "        test_indices = loaded_test_df['label'].map(label_map).values\n",
        "\n",
        "        # Appliquer to_categorical sur les indices\n",
        "        train_labels_encoded = to_categorical(train_indices, num_classes=N_CLASSES)\n",
        "        test_labels_encoded = to_categorical(test_indices, num_classes=N_CLASSES)\n",
        "\n",
        "    elif train_labels_encoded.ndim == 2 and train_labels_encoded.shape[1] == 1:\n",
        "        # Cas binaire [N, 1], le convertir en [N, 2]\n",
        "        train_labels_encoded = to_categorical(train_labels_encoded, num_classes=N_CLASSES)\n",
        "        test_labels_encoded = to_categorical(test_labels_encoded, num_classes=N_CLASSES)\n",
        "\n",
        "\n",
        "# V√âRIFICATION FINALE DE LA FORME\n",
        "if train_labels_encoded.ndim != 2 or train_labels_encoded.shape[1] != N_CLASSES:\n",
        "    print(f\"‚ùå ERREUR CRITIQUE DE FORME: Les labels ont la forme {train_labels_encoded.shape} (Rang {train_labels_encoded.ndim}), mais le mod√®le attend le rang 2 avec {N_CLASSES} colonnes.\")\n",
        "    print(\"V√©rifiez l'√©tape de sauvegarde initiale.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"[INFO] Forme finale des labels TRAIN : {train_labels_encoded.shape}\")\n",
        "print(f\"[INFO] Forme finale des labels TEST : {test_labels_encoded.shape}\")\n",
        "\n",
        "# 3. Cr√©er les g√©n√©rateurs Keras\n",
        "train_generator = DataGenerator(loaded_train_df, train_labels_encoded, batch_size=BS, n_classes=N_CLASSES, shuffle=True)\n",
        "test_generator = DataGenerator(loaded_test_df, test_labels_encoded, batch_size=BS, n_classes=N_CLASSES, shuffle=False)\n",
        "print(f\"[INFO] Train samples : {len(loaded_train_df)}\")\n",
        "print(f\"[INFO] Test samples : {len(loaded_test_df)}\")\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "#                      D√âFINITION ET ENTRA√éNEMENT\n",
        "# ====================================================================\n",
        "\n",
        "print(\"\\n## 3. D√©finition et Entra√Ænement du Mod√®le üß†\")\n",
        "# ... (D√©finition du mod√®le CNN, compilation et entra√Ænement - inchang√©)\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(WIDTH, HEIGHT, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(N_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=INIT_LR),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(\"\\n[INFO] Lancement de l'entra√Ænement sur les g√©n√©rateurs...\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=len(test_generator)\n",
        ")\n",
        "\n",
        "print(\"\\n[FIN] Entra√Ænement termin√© !\")\n",
        "\n",
        "# ====================================================================\n",
        "#                      SAUVEGARDE DU MOD√àLE\n",
        "# ====================================================================\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, 'plant_disease_model.keras')\n",
        "model.save(MODEL_PATH)\n",
        "print(f\"‚úÖ Mod√®le Keras sauvegard√© avec succ√®s √† : {MODEL_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
