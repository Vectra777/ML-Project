"""
Random Forest training script using CNN features (MobileNetV2) with streaming/cached extraction:
- Reuses the pre-split metadata from artifacts generated by load_files.py
- Extracts pooled CNN features in batches and caches them to disk (memmap) to avoid recompute
- Scales features and trains/evaluates a RandomForestClassifier with optional hyperparam search
"""

import argparse
import pickle
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    precision_score,
)
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

BASE_DIR = Path(__file__).resolve().parent
ARTIFACTS_DIR = BASE_DIR / "artifacts"
TRAIN_CSV = ARTIFACTS_DIR / "train_df.csv"
TEST_CSV = ARTIFACTS_DIR / "test_df.csv"
BINARIZER_PATH = ARTIFACTS_DIR / "label_binarizer.pkl"

MODEL_PATH = ARTIFACTS_DIR / "rf_model.pkl"
SCALER_PATH = ARTIFACTS_DIR / "rf_scaler.pkl"
CONFUSION_PATH = ARTIFACTS_DIR / "rf_confusion.csv"
REPORT_PATH = ARTIFACTS_DIR / "rf_report.txt"
CONFUSION_IMG_PATH = ARTIFACTS_DIR / "rf_confusion.png"
PRECISION_IMG_PATH = ARTIFACTS_DIR / "rf_precision.png"

# Feature extraction config (MobileNetV2 pooled features)
CNN_SIZE = (224, 224)
DEFAULT_BATCH = 64
cnn_model = MobileNetV2(weights="imagenet", include_top=False, pooling="avg", input_shape=(224, 224, 3))
FEATURE_SIZE = int(np.prod(cnn_model.output_shape[1:]))


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train a RandomForest on PlantVillage with CNN features (streamed).")
    parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH, help="Images per batch for feature extraction/scaling.")
    parser.add_argument("--search", action="store_true", help="Run RandomizedSearchCV to tune hyperparameters.")
    parser.add_argument("--n-iter", type=int, default=20, help="Number of RandomizedSearchCV iterations.")
    parser.add_argument("--n-jobs", type=int, default=-1, help="Parallelism for sklearn jobs.")
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=400,
        help="Tree count when search is disabled (ignored when --search is used).",
    )
    return parser.parse_args()


def iter_chunks(n_items: int, batch_size: int) -> Iterable[Tuple[int, int]]:
    for start in range(0, n_items, batch_size):
        end = min(start + batch_size, n_items)
        yield start, end


def load_metadata() -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, int]]:
    if not TRAIN_CSV.exists() or not TEST_CSV.exists():
        raise FileNotFoundError("Missing train/test CSVs. Run scripts/load_files.py first.")
    if not BINARIZER_PATH.exists():
        raise FileNotFoundError("Missing label binarizer. Run scripts/load_files.py first.")

    train_df = pd.read_csv(TRAIN_CSV).reset_index(drop=True)
    test_df = pd.read_csv(TEST_CSV).reset_index(drop=True)
    with open(BINARIZER_PATH, "rb") as f:
        binarizer = pickle.load(f)
    label_to_idx = {name: i for i, name in enumerate(binarizer.classes_)}
    return train_df, test_df, label_to_idx


def _existing_memmap_valid(path: Path, count: int) -> bool:
    if not path.exists():
        return False
    expected_bytes = count * FEATURE_SIZE * np.dtype(np.float32).itemsize
    return path.stat().st_size == expected_bytes


def _extract_batch_features(paths: List[str]) -> np.ndarray:
    paths = list(paths)
    batch_size = len(paths)
    imgs = np.empty((batch_size, *CNN_SIZE, 3), dtype=np.float32)
    for i, path in enumerate(paths):
        img = cv2.imread(path)
        if img is None:
            raise ValueError(f"Could not read image: {path}")
        img = cv2.resize(img, CNN_SIZE)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)
        imgs[i] = img

    imgs = preprocess_input(imgs)
    feats = cnn_model.predict(imgs, verbose=0)
    if feats.ndim > 2:
        feats = feats.reshape(batch_size, -1)
    return feats.astype(np.float32)


def build_feature_store(df: pd.DataFrame, label_map: Dict[str, int], split: str, batch_size: int) -> Tuple[Path, np.ndarray]:
    df = df.reset_index(drop=True)
    count = len(df)
    feature_path = ARTIFACTS_DIR / f"rf_{split}_features.dat"
    labels = df["label"].map(label_map).to_numpy(dtype=np.int32)

    if _existing_memmap_valid(feature_path, count):
        print(f"[INFO] Reusing cached {split} features at {feature_path}")
        return feature_path, labels

    if feature_path.exists():
        feature_path.unlink()
    features = np.memmap(feature_path, dtype=np.float32, mode="w+", shape=(count, FEATURE_SIZE))
    print(f"[INFO] Building {split} features ({count} samples) in batches of {batch_size}...")
    for idx, (start, end) in enumerate(iter_chunks(count, batch_size), start=1):
        rows = df.iloc[start:end]
        feats = _extract_batch_features(rows["path"].tolist())
        features[start:end] = feats
        if idx % 10 == 0 or end == count:
            print(f"  - Processed {end}/{count}")

    features.flush()
    return feature_path, labels


def fit_scaler(train_features_path: Path, train_count: int, batch_size: int) -> StandardScaler:
    scaler = StandardScaler()
    mmap = np.memmap(train_features_path, dtype=np.float32, mode="r", shape=(train_count, FEATURE_SIZE))
    for start, end in iter_chunks(train_count, batch_size):
        scaler.partial_fit(mmap[start:end])
    with open(SCALER_PATH, "wb") as f:
        pickle.dump(scaler, f)
    print(f"[INFO] StandardScaler fitted and saved to {SCALER_PATH}")
    return scaler


def transform_split(features_path: Path, sample_count: int, scaler: StandardScaler, split: str, batch_size: int) -> Path:
    scaled_path = ARTIFACTS_DIR / f"rf_{split}_scaled.dat"
    if scaled_path.exists():
        scaled_path.unlink()
    scaled = np.memmap(scaled_path, dtype=np.float32, mode="w+", shape=(sample_count, FEATURE_SIZE))
    src = np.memmap(features_path, dtype=np.float32, mode="r", shape=(sample_count, FEATURE_SIZE))

    for start, end in iter_chunks(sample_count, batch_size):
        scaled[start:end] = scaler.transform(src[start:end])
    scaled.flush()
    return scaled_path


def train_model(
    X_train_path: Path, y_train: np.ndarray, args: argparse.Namespace
) -> RandomForestClassifier:
    X_train = np.memmap(X_train_path, dtype=np.float32, mode="r", shape=(len(y_train), FEATURE_SIZE))
    base_model = RandomForestClassifier(random_state=42, n_jobs=args.n_jobs)

    if args.search:
        param_dist = {
            "n_estimators": [200, 400, 600, 800],
            "max_depth": [15, 25, 35, None],
            "min_samples_split": [2, 5, 10],
            "min_samples_leaf": [1, 2, 4],
            "max_features": ["sqrt", "log2"],
        }
        print("[INFO] Running RandomizedSearchCV for RandomForest...")
        search = RandomizedSearchCV(
            base_model,
            param_distributions=param_dist,
            n_iter=args.n_iter,
            cv=3,
            verbose=2,
            n_jobs=args.n_jobs,
        )
        search.fit(X_train, y_train)
        model = search.best_estimator_
        print(f"[INFO] Best parameters: {search.best_params_}")
    else:
        model = RandomForestClassifier(
            n_estimators=args.n_estimators,
            random_state=42,
            n_jobs=args.n_jobs,
        )
        print("[INFO] Training RandomForest with fixed hyperparameters...")
        model.fit(X_train, y_train)

    with open(MODEL_PATH, "wb") as f:
        pickle.dump(model, f)
    print(f"[INFO] Model saved to {MODEL_PATH}")
    return model


def evaluate(
    model: RandomForestClassifier,
    X_train_path: Path,
    y_train: np.ndarray,
    X_test_path: Path,
    y_test: np.ndarray,
    label_lookup: Dict[int, str],
) -> None:
    X_train = np.memmap(X_train_path, dtype=np.float32, mode="r", shape=(len(y_train), FEATURE_SIZE))
    X_test = np.memmap(X_test_path, dtype=np.float32, mode="r", shape=(len(y_test), FEATURE_SIZE))

    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    labels_sorted = [label_lookup[i] for i in sorted(label_lookup.keys())]
    labels_idx = list(range(len(labels_sorted)))

    acc_train = accuracy_score(y_train, y_pred_train)
    acc_test = accuracy_score(y_test, y_pred_test)
    prec_train = precision_score(y_train, y_pred_train, labels=labels_idx, average="macro", zero_division=0)
    prec_test = precision_score(y_test, y_pred_test, labels=labels_idx, average="macro", zero_division=0)

    report = classification_report(y_test, y_pred_test, labels=labels_idx, target_names=labels_sorted, digits=4)
    cm = confusion_matrix(y_test, y_pred_test, labels=labels_idx)

    with open(REPORT_PATH, "w") as f:
        f.write(f"Train accuracy: {acc_train:.4f}\n")
        f.write(f"Test accuracy:  {acc_test:.4f}\n")
        f.write(f"Train precision (macro): {prec_train:.4f}\n")
        f.write(f"Test precision  (macro): {prec_test:.4f}\n\n")
        f.write(report)
    pd.DataFrame(cm, index=labels_sorted, columns=labels_sorted).to_csv(CONFUSION_PATH)

    _plot_confusion(cm, labels_sorted, CONFUSION_IMG_PATH)
    _plot_precision(prec_train, prec_test, PRECISION_IMG_PATH)

    print(f"[INFO] Train accuracy: {acc_train * 100:.2f}% | Test accuracy: {acc_test * 100:.2f}%")
    print(f"[INFO] Train precision (macro): {prec_train:.4f} | Test precision (macro): {prec_test:.4f}")
    print(f"[INFO] Classification report written to {REPORT_PATH}")
    print(f"[INFO] Confusion matrix saved to {CONFUSION_PATH} and {CONFUSION_IMG_PATH}")
    print(f"[INFO] Precision bar chart saved to {PRECISION_IMG_PATH}")


def _plot_confusion(cm: np.ndarray, labels: list, out_path: Path) -> None:
    plt.figure(figsize=(12, 10))
    im = plt.imshow(cm, interpolation="nearest", cmap="Blues")
    plt.colorbar(im, fraction=0.046, pad=0.04)
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=90)
    plt.yticks(tick_marks, labels)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix (counts)")
    plt.tight_layout()
    plt.savefig(out_path, bbox_inches="tight")
    plt.close()


def _plot_precision(train_prec: float, test_prec: float, out_path: Path) -> None:
    plt.figure(figsize=(6, 4))
    bars = plt.bar(["Train precision", "Test precision"], [train_prec, test_prec], color=["#4C78A8", "#F58518"])
    plt.ylim(0, 1)
    for bar, val in zip(bars, [train_prec, test_prec]):
        plt.text(bar.get_x() + bar.get_width() / 2, val + 0.02, f"{val:.3f}", ha="center", va="bottom")
    plt.ylabel("Macro precision")
    plt.title("Precision comparison (train vs test)")
    plt.tight_layout()
    plt.savefig(out_path, bbox_inches="tight")
    plt.close()


def main():
    args = parse_args()
    ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

    train_df, test_df, label_map = load_metadata()
    inv_label_map = {v: k for k, v in label_map.items()}

    train_feat_path, y_train = build_feature_store(train_df, label_map, "train", args.batch_size)
    test_feat_path, y_test = build_feature_store(test_df, label_map, "test", args.batch_size)

    scaler = fit_scaler(train_feat_path, len(train_df), args.batch_size)
    train_scaled_path = transform_split(train_feat_path, len(train_df), scaler, "train", args.batch_size)
    test_scaled_path = transform_split(test_feat_path, len(test_df), scaler, "test", args.batch_size)

    model = train_model(train_scaled_path, y_train, args)
    evaluate(model, train_scaled_path, y_train, test_scaled_path, y_test, inv_label_map)


if __name__ == "__main__":
    main()
